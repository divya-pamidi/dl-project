# -*- coding: utf-8 -*-
"""t5working-3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Hw0uYaY_4y3quAfrn32PIbPbShC3HkQy
"""

# Install required packages
!pip install transformers datasets evaluate sacrebleu

# Download the Europarl dataset
!wget https://www.statmt.org/europarl/v7/fr-en.tgz
!tar -xf fr-en.tgz

import os
import re
import string
import unicodedata
import pandas as pd
import random
from datasets import Dataset
from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer
import torch

# Preprocessing functions
def load_doc(filename):
    with open(filename, mode='rt', encoding='utf-8') as file:
        return file.read()

def to_sentences(doc):
    return doc.strip().split('\n')

def clean_lines(lines):
    cleaned = []
    re_print = re.compile('[^%s]' % re.escape(string.printable))
    table = str.maketrans('', '', string.punctuation)
    for line in lines:
        line = unicodedata.normalize('NFD', line).encode('ascii', 'ignore').decode('UTF-8')
        line = line.split()
        line = [word.lower() for word in line]
        line = [word.translate(table) for word in line]
        line = [re_print.sub('', w) for w in line]
        line = [word for word in line if word.isalpha()]
        cleaned.append(' '.join(line))
    return cleaned

# Load, clean, and split the dataset
filename_en = 'europarl-v7.fr-en.en'
filename_fr = 'europarl-v7.fr-en.fr'
doc_en = load_doc(filename_en)
doc_fr = load_doc(filename_fr)
sentences_en = to_sentences(doc_en)
sentences_fr = to_sentences(doc_fr)
sentences_en = clean_lines(sentences_en)
sentences_fr = clean_lines(sentences_fr)

MAX_LENGTH = 10

# Combine into a single DataFrame
df = pd.DataFrame({'English': sentences_en, 'French': sentences_fr})

# Filter the DataFrame based on the max_length of sentences
df = df[df['English'].str.split().str.len().le(MAX_LENGTH) & df['French'].str.split().str.len().le(MAX_LENGTH)]



# Split data into train, validation, and test sets
train_df, test_df = train_test_split(df, test_size=0.2)
train_df, eval_df = train_test_split(train_df, test_size=0.1)

# Convert DataFrame to Hugging Face Dataset
train_dataset = Dataset.from_pandas(train_df)
eval_dataset = Dataset.from_pandas(eval_df)
test_dataset = Dataset.from_pandas(test_df)



# Initialize tokenizer and model
checkpoint = "t5-base"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)

# Preprocess the data
def preprocess_function(examples):
    inputs = ["translate English to French: " + ex for ex in examples['English']]
    targets = examples['French']
    model_inputs = tokenizer(inputs, max_length=128, truncation=True)
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(targets, max_length=128, truncation=True)
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# Tokenize the dataset
tokenized_train = train_dataset.map(preprocess_function, batched=True)
tokenized_eval = eval_dataset.map(preprocess_function, batched=True)
tokenized_test = test_dataset.map(preprocess_function, batched=True)

total_train_examples = len(train_dataset)  # Replace with the actual size of your training dataset
train_batch_size = 8  # As defined in your training arguments

# Calculate the number of steps in one epoch
steps_per_epoch = total_train_examples // train_batch_size

# Set save_steps to twice the number of steps per epoch
save_steps = 2 * steps_per_epoch

# Training arguments
training_args = Seq2SeqTrainingArguments(
    output_dir="europarl_translator",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=train_batch_size,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir='./logs',
    save_strategy="steps",
    save_steps=save_steps  # Save the model every 2 epochs
)

# Now use these training_args in your Seq2SeqTrainer

# Initialize the trainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_eval,
    tokenizer=tokenizer,
    data_collator=DataCollatorForSeq2Seq(tokenizer)
)

# Train and evaluate the model
trainer.train()

# Evaluate on test dataset
test_results = trainer.evaluate(eval_dataset=tokenized_test)
print("Test Results:", test_results)

def translate_random_sentences(dataset, n=10):
    model.to("cuda")  # Move the model to GPU. Use model.to("cpu") if you don't have a GPU.

    sample_indices = random.sample(range(len(dataset)), n)
    examples = [dataset[i] for i in sample_indices]

    for example in examples:
        original_english = example['English']
        actual_french = example['French']

        inputs = tokenizer("translate English to French: " + original_english, return_tensors="pt", max_length=128, truncation=True)
        inputs = {k: v.to("cuda") for k, v in inputs.items()}  # Use .to("cpu") if you don't have a GPU.

        translated_ids = model.generate(**inputs)
        translated_french = tokenizer.decode(translated_ids[0], skip_special_tokens=True)

        print(f"Original English: {original_english}")
        print(f"Actual French: {actual_french}")
        print(f"Translated French: {translated_french}\n")

# Translate random sentences from test data
translate_random_sentences(test_dataset)

# Initialize the trainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_eval,
    tokenizer=tokenizer,
    data_collator=DataCollatorForSeq2Seq(tokenizer)
)

# Train and evaluate the model
trainer.train()

# Evaluate on test dataset
test_results = trainer.evaluate(eval_dataset=tokenized_test)
print("Test Results:", test_results)

from sacrebleu.metrics import BLEU

bleu = BLEU()

def translate_and_evaluate_random_sentences(dataset, n=10):
    model.to("cuda")  # Move the model to GPU. Use model.to("cpu") if you don't have a GPU.

    sample_indices = random.sample(range(len(dataset)), n)
    examples = [dataset[i] for i in sample_indices]
    translations = []

    for example in examples:
        original_english = example['English']
        actual_french = example['French']

        inputs = tokenizer("translate English to French: " + original_english, return_tensors="pt", max_length=128, truncation=True)
        inputs = {k: v.to("cuda") for k, v in inputs.items()}  # Use .to("cpu") if you don't have a GPU.

        translated_ids = model.generate(**inputs)
        translated_french = tokenizer.decode(translated_ids[0], skip_special_tokens=True)

        # Calculate BLEU score
        score = bleu.sentence_score(translated_french, [actual_french]).score
        translations.append((original_english, actual_french, translated_french, score))

    # Sort by BLEU score and select top 5
    top_translations = sorted(translations, key=lambda x: x[3], reverse=True)[:5]

    # Print the top 5 translations
    for original, actual, translation, score in top_translations:
        print(f"Original English: {original}")
        print(f"Actual French: {actual}")
        print(f"Translated French: {translation}")
        print(f"BLEU Score: {score}\n")

# Translate and evaluate random sentences from test data
translate_and_evaluate_random_sentences(test_dataset)

# Commented out IPython magic to ensure Python compatibility.
# %env TOKENIZERS_PARALLELISM=false

!zip -r europarl_translator_model.zip /kaggle/working/europarl_translator_model

# Example: Deleting a large, unnecessary file
!rm /kaggle/working/europarl-v7.fr-en.en

# Then try to compress your model again
!zip -r europarl_translator_model.zip /kaggle/working/europarl_translator_model

