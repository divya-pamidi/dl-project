{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-12-10T12:20:53.002745Z","iopub.status.busy":"2023-12-10T12:20:53.002431Z","iopub.status.idle":"2023-12-10T12:21:02.177132Z","shell.execute_reply":"2023-12-10T12:21:02.176068Z","shell.execute_reply.started":"2023-12-10T12:20:53.002714Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["--2023-12-10 12:20:53--  https://www.statmt.org/europarl/v7/fr-en.tgz\n","Resolving www.statmt.org (www.statmt.org)... 129.215.32.28\n","Connecting to www.statmt.org (www.statmt.org)|129.215.32.28|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 202718517 (193M) [application/x-gzip]\n","Saving to: ‘fr-en.tgz’\n","\n","fr-en.tgz           100%[===================>] 193.33M   101MB/s    in 1.9s    \n","\n","2023-12-10 12:20:55 (101 MB/s) - ‘fr-en.tgz’ saved [202718517/202718517]\n","\n"]}],"source":["!wget https://www.statmt.org/europarl/v7/fr-en.tgz\n","!tar -xf fr-en.tgz"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T12:21:02.179854Z","iopub.status.busy":"2023-12-10T12:21:02.179544Z","iopub.status.idle":"2023-12-10T12:21:04.322893Z","shell.execute_reply":"2023-12-10T12:21:04.321714Z","shell.execute_reply.started":"2023-12-10T12:21:02.179825Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["--2023-12-10 12:21:03--  https://download.pytorch.org/tutorial/data.zip\n","Resolving download.pytorch.org (download.pytorch.org)... 18.239.83.69, 18.239.83.32, 18.239.83.126, ...\n","Connecting to download.pytorch.org (download.pytorch.org)|18.239.83.69|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2882130 (2.7M) [application/zip]\n","Saving to: ‘data.zip’\n","\n","data.zip            100%[===================>]   2.75M  --.-KB/s    in 0.03s   \n","\n","2023-12-10 12:21:03 (86.7 MB/s) - ‘data.zip’ saved [2882130/2882130]\n","\n","Archive:  data.zip\n","   creating: data/\n","  inflating: data/eng-fra.txt        \n","   creating: data/names/\n","  inflating: data/names/Arabic.txt   \n","  inflating: data/names/Chinese.txt  \n","  inflating: data/names/Czech.txt    \n","  inflating: data/names/Dutch.txt    \n","  inflating: data/names/English.txt  \n","  inflating: data/names/French.txt   \n","  inflating: data/names/German.txt   \n","  inflating: data/names/Greek.txt    \n","  inflating: data/names/Irish.txt    \n","  inflating: data/names/Italian.txt  \n","  inflating: data/names/Japanese.txt  \n","  inflating: data/names/Korean.txt   \n","  inflating: data/names/Polish.txt   \n","  inflating: data/names/Portuguese.txt  \n","  inflating: data/names/Russian.txt  \n","  inflating: data/names/Scottish.txt  \n","  inflating: data/names/Spanish.txt  \n","  inflating: data/names/Vietnamese.txt  \n"]}],"source":["!wget https://download.pytorch.org/tutorial/data.zip\n","!unzip data.zip"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T12:21:04.324553Z","iopub.status.busy":"2023-12-10T12:21:04.324254Z","iopub.status.idle":"2023-12-10T12:23:38.278649Z","shell.execute_reply":"2023-12-10T12:23:38.277671Z","shell.execute_reply.started":"2023-12-10T12:21:04.324524Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Saved: english.pkl\n","resumption of the session\n","i declare resumed the session of the european parliament adjourned on friday december and i would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period\n","although as you will have seen the dreaded millennium bug failed to materialise still the people in a number of countries suffered a series of natural disasters that truly were dreadful\n","you have requested a debate on this subject in the course of the next few days during this partsession\n","in the meantime i should like to observe a minute s silence as a number of members have requested on behalf of all the victims concerned particularly those of the terrible storms in the various countries of the european union\n","please rise then for this minute s silence\n","the house rose and observed a minute s silence\n","madam president on a point of order\n","you will be aware from the press and television that there have been a number of bomb explosions and killings in sri lanka\n","one of the people assassinated very recently in sri lanka was mr kumar ponnambalam who had visited the european parliament just a few months ago\n","Saved: french.pkl\n","reprise de la session\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>English</th>\n","      <th>French</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>resumption of the session</td>\n","      <td>reprise de la session</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>i declare resumed the session of the european ...</td>\n","      <td>je declare reprise la session du parlement eur...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>although as you will have seen the dreaded mil...</td>\n","      <td>comme vous avez pu le constater le grand bogue...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>you have requested a debate on this subject in...</td>\n","      <td>vous avez souhaite un debat a ce sujet dans le...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>in the meantime i should like to observe a min...</td>\n","      <td>en attendant je souhaiterais comme un certain ...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2007718</th>\n","      <td>i would also like although they are absent to ...</td>\n","      <td>je me permettrai meme bien quils soient absent...</td>\n","    </tr>\n","    <tr>\n","      <th>2007719</th>\n","      <td>i am not going to reopen the millennium or not...</td>\n","      <td>je ne rouvrirai pas le debat sur le millenaire...</td>\n","    </tr>\n","    <tr>\n","      <th>2007720</th>\n","      <td>adjournment of the session</td>\n","      <td>interruption de la session</td>\n","    </tr>\n","    <tr>\n","      <th>2007721</th>\n","      <td>i declare the session of the european parliame...</td>\n","      <td>je declare interrompue la session du parlement...</td>\n","    </tr>\n","    <tr>\n","      <th>2007722</th>\n","      <td>the sitting was closed at am</td>\n","      <td>la seance est levee a</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2007723 rows × 2 columns</p>\n","</div>"],"text/plain":["                                                   English  \\\n","0                                resumption of the session   \n","1        i declare resumed the session of the european ...   \n","2        although as you will have seen the dreaded mil...   \n","3        you have requested a debate on this subject in...   \n","4        in the meantime i should like to observe a min...   \n","...                                                    ...   \n","2007718  i would also like although they are absent to ...   \n","2007719  i am not going to reopen the millennium or not...   \n","2007720                         adjournment of the session   \n","2007721  i declare the session of the european parliame...   \n","2007722                       the sitting was closed at am   \n","\n","                                                    French  \n","0                                    reprise de la session  \n","1        je declare reprise la session du parlement eur...  \n","2        comme vous avez pu le constater le grand bogue...  \n","3        vous avez souhaite un debat a ce sujet dans le...  \n","4        en attendant je souhaiterais comme un certain ...  \n","...                                                    ...  \n","2007718  je me permettrai meme bien quils soient absent...  \n","2007719  je ne rouvrirai pas le debat sur le millenaire...  \n","2007720                         interruption de la session  \n","2007721  je declare interrompue la session du parlement...  \n","2007722                              la seance est levee a  \n","\n","[2007723 rows x 2 columns]"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["import re\n","import string\n","import unicodedata\n","import pickle\n","import pandas as pd \n","# load doc into memory\n","def load_doc(filename):\n","\t# open the file as read only\n","\tfile = open(filename, mode='rt', encoding='utf-8')\n","\t# read all text\n","\ttext = file.read()\n","\t# close the file\n","\tfile.close()\n","\treturn text\n","\n","# split a loaded document into sentences\n","def to_sentences(doc):\n","\treturn doc.strip().split('\\n')\n","\n","# clean a list of lines\n","def clean_lines(lines):\n","\tcleaned = list()\n","\t# prepare regex for char filtering\n","\tre_print = re.compile('[^%s]' % re.escape(string.printable))\n","\t# prepare translation table for removing punctuation\n","\ttable = str.maketrans('', '', string.punctuation)\n","\tfor line in lines:\n","\t\t# normalize unicode characters\n","\t\tline = unicodedata.normalize('NFD', line).encode('ascii', 'ignore')\n","\t\tline = line.decode('UTF-8')\n","\t\t# tokenize on white space\n","\t\tline = line.split()\n","\t\t# convert to lower case\n","\t\tline = [word.lower() for word in line]\n","\t\t# remove punctuation from each token\n","\t\tline = [word.translate(table) for word in line]\n","\t\t# remove non-printable chars form each token\n","\t\tline = [re_print.sub('', w) for w in line]\n","\t\t# remove tokens with numbers in them\n","\t\tline = [word for word in line if word.isalpha()]\n","\t\t# store as string\n","\t\tcleaned.append(' '.join(line))\n","\treturn cleaned\n","\n","# save a list of clean sentences to file\n","def save_clean_sentences(sentences, filename):\n","\tpickle.dump(sentences, open(filename, 'wb'))\n","\tprint('Saved: %s' % filename)\n","\n","# load English data\n","filename = 'europarl-v7.fr-en.en'\n","doc = load_doc(filename)\n","sentences = to_sentences(doc)\n","sentences = clean_lines(sentences)\n","save_clean_sentences(sentences, 'english.pkl')\n","# spot check\n","for i in range(10):\n","\tprint(sentences[i])\n","\n","# load French data\n","filename = 'europarl-v7.fr-en.fr'\n","doc = load_doc(filename)\n","sentences = to_sentences(doc)\n","sentences = clean_lines(sentences)\n","save_clean_sentences(sentences, 'french.pkl')\n","# spot check\n","for i in range(1):\n","\tprint(sentences[i])\n","#This will take our WMT2014 datasets and clean them of any punctuation, uppercase letters, non-printable characters, and tokens with numbers in them. Then it pickles the files for later use.\n","\n","with open('french.pkl', 'rb') as f:\n","    fr_voc = pickle.load(f)\n","\n","with open('english.pkl', 'rb') as f:\n","    eng_voc = pickle.load(f)\n","    \n","data = pd.DataFrame(zip(eng_voc, fr_voc), columns = ['English', 'French'])\n","data"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T12:23:38.280215Z","iopub.status.busy":"2023-12-10T12:23:38.279816Z","iopub.status.idle":"2023-12-10T12:23:38.499431Z","shell.execute_reply":"2023-12-10T12:23:38.498538Z","shell.execute_reply.started":"2023-12-10T12:23:38.280189Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>English</th>\n","      <th>French</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Go.</td>\n","      <td>Va !</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Run!</td>\n","      <td>Cours !</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Run!</td>\n","      <td>Courez !</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Wow!</td>\n","      <td>Ça alors !</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Fire!</td>\n","      <td>Au feu !</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>135837</th>\n","      <td>A carbon footprint is the amount of carbon dio...</td>\n","      <td>Une empreinte carbone est la somme de pollutio...</td>\n","    </tr>\n","    <tr>\n","      <th>135838</th>\n","      <td>Death is something that we're often discourage...</td>\n","      <td>La mort est une chose qu'on nous décourage sou...</td>\n","    </tr>\n","    <tr>\n","      <th>135839</th>\n","      <td>Since there are usually multiple websites on a...</td>\n","      <td>Puisqu'il y a de multiples sites web sur chaqu...</td>\n","    </tr>\n","    <tr>\n","      <th>135840</th>\n","      <td>If someone who doesn't know your background sa...</td>\n","      <td>Si quelqu'un qui ne connaît pas vos antécédent...</td>\n","    </tr>\n","    <tr>\n","      <th>135841</th>\n","      <td>It may be impossible to get a completely error...</td>\n","      <td>Il est peut-être impossible d'obtenir un Corpu...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>135842 rows × 2 columns</p>\n","</div>"],"text/plain":["                                                  English  \\\n","0                                                     Go.   \n","1                                                    Run!   \n","2                                                    Run!   \n","3                                                    Wow!   \n","4                                                   Fire!   \n","...                                                   ...   \n","135837  A carbon footprint is the amount of carbon dio...   \n","135838  Death is something that we're often discourage...   \n","135839  Since there are usually multiple websites on a...   \n","135840  If someone who doesn't know your background sa...   \n","135841  It may be impossible to get a completely error...   \n","\n","                                                   French  \n","0                                                    Va !  \n","1                                                 Cours !  \n","2                                                Courez !  \n","3                                              Ça alors !  \n","4                                                Au feu !  \n","...                                                   ...  \n","135837  Une empreinte carbone est la somme de pollutio...  \n","135838  La mort est une chose qu'on nous décourage sou...  \n","135839  Puisqu'il y a de multiples sites web sur chaqu...  \n","135840  Si quelqu'un qui ne connaît pas vos antécédent...  \n","135841  Il est peut-être impossible d'obtenir un Corpu...  \n","\n","[135842 rows x 2 columns]"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["data2 = pd.read_csv('/kaggle/working/data/eng-fra.txt', delimiter='\\t', names = ['English', 'French'])\n","data2"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T12:23:38.500929Z","iopub.status.busy":"2023-12-10T12:23:38.500599Z","iopub.status.idle":"2023-12-10T12:24:09.897573Z","shell.execute_reply":"2023-12-10T12:24:09.896660Z","shell.execute_reply.started":"2023-12-10T12:23:38.500869Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>English</th>\n","      <th>French</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>resumption of the session</td>\n","      <td>reprise de la session</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>i declare resumed the session of the european ...</td>\n","      <td>je declare reprise la session du parlement eur...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>although as you will have seen the dreaded mil...</td>\n","      <td>comme vous avez pu le constater le grand bogue...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>you have requested a debate on this subject in...</td>\n","      <td>vous avez souhaite un debat a ce sujet dans le...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>in the meantime i should like to observe a min...</td>\n","      <td>en attendant je souhaiterais comme un certain ...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2143560</th>\n","      <td>A carbon footprint is the amount of carbon dio...</td>\n","      <td>Une empreinte carbone est la somme de pollutio...</td>\n","    </tr>\n","    <tr>\n","      <th>2143561</th>\n","      <td>Death is something that we're often discourage...</td>\n","      <td>La mort est une chose qu'on nous décourage sou...</td>\n","    </tr>\n","    <tr>\n","      <th>2143562</th>\n","      <td>Since there are usually multiple websites on a...</td>\n","      <td>Puisqu'il y a de multiples sites web sur chaqu...</td>\n","    </tr>\n","    <tr>\n","      <th>2143563</th>\n","      <td>If someone who doesn't know your background sa...</td>\n","      <td>Si quelqu'un qui ne connaît pas vos antécédent...</td>\n","    </tr>\n","    <tr>\n","      <th>2143564</th>\n","      <td>It may be impossible to get a completely error...</td>\n","      <td>Il est peut-être impossible d'obtenir un Corpu...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2143565 rows × 2 columns</p>\n","</div>"],"text/plain":["                                                   English  \\\n","0                                resumption of the session   \n","1        i declare resumed the session of the european ...   \n","2        although as you will have seen the dreaded mil...   \n","3        you have requested a debate on this subject in...   \n","4        in the meantime i should like to observe a min...   \n","...                                                    ...   \n","2143560  A carbon footprint is the amount of carbon dio...   \n","2143561  Death is something that we're often discourage...   \n","2143562  Since there are usually multiple websites on a...   \n","2143563  If someone who doesn't know your background sa...   \n","2143564  It may be impossible to get a completely error...   \n","\n","                                                    French  \n","0                                    reprise de la session  \n","1        je declare reprise la session du parlement eur...  \n","2        comme vous avez pu le constater le grand bogue...  \n","3        vous avez souhaite un debat a ce sujet dans le...  \n","4        en attendant je souhaiterais comme un certain ...  \n","...                                                    ...  \n","2143560  Une empreinte carbone est la somme de pollutio...  \n","2143561  La mort est une chose qu'on nous décourage sou...  \n","2143562  Puisqu'il y a de multiples sites web sur chaqu...  \n","2143563  Si quelqu'un qui ne connaît pas vos antécédent...  \n","2143564  Il est peut-être impossible d'obtenir un Corpu...  \n","\n","[2143565 rows x 2 columns]"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["data = pd.concat([data,data2], ignore_index= True, axis = 0)\n","\n","data.to_csv('eng-fra.txt')\n","data"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T12:24:09.898992Z","iopub.status.busy":"2023-12-10T12:24:09.898701Z","iopub.status.idle":"2023-12-10T12:24:13.022008Z","shell.execute_reply":"2023-12-10T12:24:13.021190Z","shell.execute_reply.started":"2023-12-10T12:24:09.898969Z"},"trusted":true},"outputs":[],"source":["from __future__ import unicode_literals, print_function, division\n","from io import open\n","import unicodedata\n","import string\n","import re\n","import random\n","\n","import torch\n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","\n","import torchtext\n","from torchtext.data import get_tokenizer\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T12:24:13.023606Z","iopub.status.busy":"2023-12-10T12:24:13.023161Z","iopub.status.idle":"2023-12-10T12:24:13.030793Z","shell.execute_reply":"2023-12-10T12:24:13.029890Z","shell.execute_reply.started":"2023-12-10T12:24:13.023580Z"},"trusted":true},"outputs":[],"source":["SOS_token = 0\n","EOS_token = 1\n","\n","\n","class Lang:\n","    def __init__(self, name):\n","        self.name = name\n","        self.word2index = {}\n","        self.word2count = {}\n","        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n","        self.n_words = 2  # Count SOS and EOS\n","\n","    def addSentence(self, sentence):\n","        for word in sentence.split(' '):\n","            self.addWord(word)\n","\n","    def addWord(self, word):\n","        if word not in self.word2index:\n","            self.word2index[word] = self.n_words\n","            self.word2count[word] = 1\n","            self.index2word[self.n_words] = word\n","            self.n_words += 1\n","        else:\n","            self.word2count[word] += 1"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T12:24:13.034227Z","iopub.status.busy":"2023-12-10T12:24:13.033951Z","iopub.status.idle":"2023-12-10T12:24:13.042768Z","shell.execute_reply":"2023-12-10T12:24:13.041847Z","shell.execute_reply.started":"2023-12-10T12:24:13.034203Z"},"trusted":true},"outputs":[],"source":["import re\n","\n","def normalizeString(s):\n","    s = s.lower().strip()\n","    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n","    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n","    return s\n","\n","\n","def readLangs(lang1, lang2, reverse=False):\n","    print(\"Reading lines...\")\n","\n","    # Read the file and split into lines\n","    lines = open('%s-%s.txt' % (lang1, lang2), encoding='utf-8').read().strip().split('\\n')\n","    \n","    #Split every line into pairs and normalize\n","    #pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n","    pairs = []\n","    #print(lines[0])\n","    for line in lines:\n","        \n","        parts = line.split(',')[1:]\n","        #print(len(parts))\n","        if len(parts) == 2:\n","            #print(pairs)\n","            pairs.append([normalizeString(s) for s in parts])\n","\n","\n","    # Reverse pairs, make Lang instances\n","    if reverse:\n","        pairs = [list(reversed(p)) for p in pairs]\n","        input_lang = Lang(lang2)\n","        output_lang = Lang(lang1)\n","    else:\n","        input_lang = Lang(lang1)\n","        output_lang = Lang(lang2)\n","\n","    return input_lang, output_lang, pairs"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T12:24:13.044038Z","iopub.status.busy":"2023-12-10T12:24:13.043773Z","iopub.status.idle":"2023-12-10T12:24:13.061414Z","shell.execute_reply":"2023-12-10T12:24:13.060567Z","shell.execute_reply.started":"2023-12-10T12:24:13.044015Z"},"trusted":true},"outputs":[{"data":{"text/plain":["('i am ',\n"," 'i m ',\n"," 'he is',\n"," 'he s ',\n"," 'she is',\n"," 'she s ',\n"," 'you are',\n"," 'you re ',\n"," 'we are',\n"," 'we re ',\n"," 'they are',\n"," 'they re ',\n"," 'i don t',\n"," 'do you',\n"," 'i want',\n"," 'are you',\n"," 'i have',\n"," 'i think',\n"," 'i can t',\n"," 'i was',\n"," 'he is',\n"," 'i m not',\n"," 'this is',\n"," 'i just',\n"," 'i didn t',\n"," 'i am',\n"," 'i thought',\n"," 'i know',\n"," 'tom is',\n"," 'i had',\n"," 'did you',\n"," 'have you',\n"," 'can you',\n"," 'he was',\n"," 'you don t',\n"," 'i d like',\n"," 'it was',\n"," 'you should',\n"," 'would you',\n"," 'i like',\n"," 'it is',\n"," 'she is',\n"," 'you can t',\n"," 'he has',\n"," 'what do',\n"," 'if you',\n"," 'i need',\n"," 'no one',\n"," 'you are',\n"," 'you have',\n"," 'i feel',\n"," 'i really',\n"," 'why don t',\n"," 'i hope',\n"," 'i will',\n"," 'we have',\n"," 'you re not',\n"," 'you re very',\n"," 'she was',\n"," 'i love',\n"," 'you must',\n"," 'i can')"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["MAX_LENGTH = 12\n","\n","eng_prefixes = [\n","    \"i am \", \"i m \",\n","    \"he is\", \"he s \",\n","    \"she is\", \"she s \",\n","    \"you are\", \"you re \",\n","    \"we are\", \"we re \",\n","    \"they are\", \"they re \", \"I don t\", \"Do you\", \"I want\", \"Are you\", \"I have\", \"I think\",\n","       \"I can t\", \"I was\", \"He is\", \"I m not\", \"This is\", \"I just\", \"I didn t\",\n","       \"I am\", \"I thought\", \"I know\", \"Tom is\", \"I had\", \"Did you\", \"Have you\",\n","       \"Can you\", \"He was\", \"You don t\", \"I d like\", \"It was\", \"You should\",\n","       \"Would you\", \"I like\", \"It is\", \"She is\", \"You can t\", \"He has\",\n","       \"What do\", \"If you\", \"I need\", \"No one\", \"You are\", \"You have\",\n","       \"I feel\", \"I really\", \"Why don t\", \"I hope\", \"I will\", \"We have\",\n","       \"You re not\", \"You re very\", \"She was\", \"I love\", \"You must\", \"I can\"]\n","eng_prefixes = (map(lambda x: x.lower(), eng_prefixes))\n","#eng_prefixes = set(eng_prefixes)\n","eng_prefixes = tuple(eng_prefixes)\n","\n","def filterPair(p):\n","    return len(p[0].split(' ')) < MAX_LENGTH and \\\n","        len(p[1].split(' ')) < MAX_LENGTH and \\\n","        p[1].startswith(eng_prefixes)\n","\n","\n","def filterPairs(pairs):\n","    return [pair for pair in pairs if filterPair(pair)]\n","eng_prefixes"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T12:24:13.062730Z","iopub.status.busy":"2023-12-10T12:24:13.062478Z","iopub.status.idle":"2023-12-10T12:25:54.777407Z","shell.execute_reply":"2023-12-10T12:25:54.776500Z","shell.execute_reply.started":"2023-12-10T12:24:13.062707Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Reading lines...\n","Read 2132020 sentence pairs\n","Trimmed to 54225 sentence pairs\n","Counting words...\n","Counted words:\n","fra 15457\n","eng 10418\n","['cest incontestable', 'this is not in dispute']\n"]}],"source":["def prepareData(lang1, lang2,reverse = False):\n","    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n","    print(\"Read %s sentence pairs\" % len(pairs))\n","    pairs = filterPairs(pairs)\n","    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n","    print(\"Counting words...\")\n","    for pair in pairs:\n","        input_lang.addSentence(pair[0])\n","        output_lang.addSentence(pair[1])\n","    print(\"Counted words:\")\n","    print(input_lang.name, input_lang.n_words)\n","    print(output_lang.name, output_lang.n_words)\n","    return input_lang, output_lang, pairs\n","\n","input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n","if len(pairs) > 0:\n","    print(random.choice(pairs))\n","#print(random.choice(pairs))"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T12:25:54.778682Z","iopub.status.busy":"2023-12-10T12:25:54.778425Z","iopub.status.idle":"2023-12-10T12:25:54.783078Z","shell.execute_reply":"2023-12-10T12:25:54.782107Z","shell.execute_reply.started":"2023-12-10T12:25:54.778660Z"},"trusted":true},"outputs":[],"source":["from tqdm import tqdm"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T12:25:54.784442Z","iopub.status.busy":"2023-12-10T12:25:54.784208Z","iopub.status.idle":"2023-12-10T12:25:54.795823Z","shell.execute_reply":"2023-12-10T12:25:54.795000Z","shell.execute_reply.started":"2023-12-10T12:25:54.784421Z"},"trusted":true},"outputs":[],"source":["class EncoderRNN(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(EncoderRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.embedding = nn.Embedding(input_size, hidden_size)\n","        self.gru = nn.GRU(hidden_size, hidden_size // 2, bidirectional=True)\n","\n","    def forward(self, input, hidden):\n","        embedded = self.embedding(input).view(1, 1, -1)\n","        output, hidden = self.gru(embedded, hidden)\n","        return output, hidden\n","\n","    def initHidden(self):\n","        return torch.zeros(2, 1, self.hidden_size // 2, device=device)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T12:25:54.797255Z","iopub.status.busy":"2023-12-10T12:25:54.796941Z","iopub.status.idle":"2023-12-10T12:25:54.810344Z","shell.execute_reply":"2023-12-10T12:25:54.809515Z","shell.execute_reply.started":"2023-12-10T12:25:54.797231Z"},"trusted":true},"outputs":[],"source":["class DecoderRNN(nn.Module):\n","    def __init__(self, hidden_size, output_size):\n","        super(DecoderRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.embedding = nn.Embedding(output_size, hidden_size)\n","        self.gru = nn.GRU(hidden_size, hidden_size)\n","        self.out = nn.Linear(hidden_size, output_size)\n","        self.softmax = nn.LogSoftmax(dim=1)\n","\n","    def forward(self, input, hidden):\n","        output = self.embedding(input).view(1, 1, -1)\n","        output = F.relu(output)\n","        output, hidden = self.gru(output, hidden)\n","        output = self.softmax(self.out(output[0]))\n","        return output, hidden\n","\n","    def initHidden(self):\n","        return torch.zeros(1, 1, self.hidden_size, device=device)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T12:25:54.811599Z","iopub.status.busy":"2023-12-10T12:25:54.811350Z","iopub.status.idle":"2023-12-10T12:25:55.266828Z","shell.execute_reply":"2023-12-10T12:25:55.266061Z","shell.execute_reply.started":"2023-12-10T12:25:54.811577Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]}],"source":["from sklearn.model_selection import train_test_split\n"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T12:25:55.268450Z","iopub.status.busy":"2023-12-10T12:25:55.268119Z","iopub.status.idle":"2023-12-10T12:25:55.273433Z","shell.execute_reply":"2023-12-10T12:25:55.272420Z","shell.execute_reply.started":"2023-12-10T12:25:55.268427Z"},"trusted":true},"outputs":[],"source":["def split_dataset(pairs, test_size=0.3, val_size=0.5):\n","    train_pairs, test_pairs = train_test_split(pairs, test_size=test_size, random_state=42)\n","    val_pairs, test_pairs = train_test_split(test_pairs, test_size=val_size, random_state=42)\n","    \n","    return train_pairs, val_pairs, test_pairs"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T12:25:55.274791Z","iopub.status.busy":"2023-12-10T12:25:55.274483Z","iopub.status.idle":"2023-12-10T12:25:55.285516Z","shell.execute_reply":"2023-12-10T12:25:55.284685Z","shell.execute_reply.started":"2023-12-10T12:25:55.274767Z"},"trusted":true},"outputs":[],"source":["def evaluate(encoder, decoder, pairs, criterion, max_length=MAX_LENGTH):\n","    with torch.no_grad():\n","        total_loss = 0\n","        correct = 0\n","        total = 0\n","\n","        for pair in pairs:\n","            input_tensor, target_tensor = tensorsFromPair(pair)\n","            input_length = input_tensor.size(0)\n","            target_length = target_tensor.size(0)\n","\n","            encoder_hidden = encoder.initHidden()\n","\n","            for ei in range(input_length):\n","                encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n","\n","            # Adjust the hidden state for the decoder: concatenate the forward and backward states\n","            decoder_hidden = torch.cat((encoder_hidden[0:1], encoder_hidden[1:2]), 2)\n","\n","            decoder_input = torch.tensor([[SOS_token]], device=device)\n","\n","            for di in range(target_length):\n","                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n","                topv, topi = decoder_output.topk(1)\n","\n","                if topi.squeeze().item() == target_tensor[di].item():\n","                    correct += 1\n","                total_loss += criterion(decoder_output, target_tensor[di]).item()\n","                decoder_input = topi.squeeze().detach()\n","\n","                if decoder_input.item() == EOS_token:\n","                    break\n","\n","            total += target_length\n","\n","        avg_loss = total_loss / total\n","        accuracy = correct / total\n","        return avg_loss, accuracy\n"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T12:25:55.288527Z","iopub.status.busy":"2023-12-10T12:25:55.287638Z","iopub.status.idle":"2023-12-10T12:25:55.299704Z","shell.execute_reply":"2023-12-10T12:25:55.299003Z","shell.execute_reply.started":"2023-12-10T12:25:55.288494Z"},"trusted":true},"outputs":[],"source":["teacher_forcing_ratio = 0.5\n","\n","\n","def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n","    encoder_hidden = encoder.initHidden()\n","\n","    encoder_optimizer.zero_grad()\n","    decoder_optimizer.zero_grad()\n","\n","    input_length = input_tensor.size(0)\n","    target_length = target_tensor.size(0)\n","\n","    loss = 0\n","    correct = 0\n","\n","    for ei in range(input_length):\n","        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n","\n","    # Adjust the hidden state for the decoder\n","    decoder_hidden = (torch.cat((encoder_hidden[0][0:1], encoder_hidden[0][1:2]), 2),\n","                      torch.cat((encoder_hidden[1][0:1], encoder_hidden[1][1:2]), 2))\n","\n","    decoder_input = torch.tensor([[SOS_token]], device=device)\n","\n","    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n","\n","    for di in range(target_length):\n","        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n","        topv, topi = decoder_output.topk(1)\n","\n","        if topi.squeeze().item() == target_tensor[di].item():\n","            correct += 1\n","\n","        loss += criterion(decoder_output, target_tensor[di])\n","        if use_teacher_forcing:\n","            decoder_input = target_tensor[di]  # Teacher forcing\n","        else:\n","            decoder_input = topi.squeeze().detach()\n","\n","        if decoder_input.item() == EOS_token:\n","            break\n","\n","    loss.backward()\n","\n","    encoder_optimizer.step()\n","    decoder_optimizer.step()\n","\n","    accuracy = correct / float(target_length)\n","    return loss.item() / target_length, accuracy\n"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T12:25:55.302240Z","iopub.status.busy":"2023-12-10T12:25:55.300658Z","iopub.status.idle":"2023-12-10T12:25:55.311085Z","shell.execute_reply":"2023-12-10T12:25:55.310316Z","shell.execute_reply.started":"2023-12-10T12:25:55.302215Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T12:29:35.207717Z","iopub.status.busy":"2023-12-10T12:29:35.206799Z","iopub.status.idle":"2023-12-10T12:29:35.230604Z","shell.execute_reply":"2023-12-10T12:29:35.229420Z","shell.execute_reply.started":"2023-12-10T12:29:35.207685Z"},"trusted":true},"outputs":[],"source":["import time\n","import math\n","\n","\n","def timeSince(since, percent):\n","    now = time.time()\n","    elapsed = now - since\n","    remaining = elapsed / percent - elapsed\n","    return '%s (- %s)' % (asMinutes(elapsed), asMinutes(remaining))\n","\n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return '%dm %ds' % (m, s)\n","\n","\n","def tensorFromSentence(lang, sentence):\n","    # Implement the logic to convert a sentence to a tensor\n","    # For example:\n","    indexes = [lang.word2index[word] for word in sentence.split()]\n","    indexes.append(EOS_token)\n","    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n","\n","\n","def tensorsFromPair(pair):\n","    # Implement the logic to convert a pair to tensors\n","    # For example:\n","    input_tensor = tensorFromSentence(input_lang, pair[0])\n","    target_tensor = tensorFromSentence(output_lang, pair[1])\n","    return (input_tensor, target_tensor)\n","\n","def plot_metrics(training_losses, validation_losses, training_accuracies, validation_accuracies):\n","    plt.figure(figsize=(12, 6))\n","\n","    plt.subplot(1, 2, 1)\n","    plt.plot(training_losses, label='Training Loss')\n","    plt.plot(validation_losses, label='Validation Loss')\n","    plt.title('Training & Validation Loss')\n","    plt.xlabel('Iterations')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","\n","    plt.subplot(1, 2, 2)\n","    plt.plot(training_accuracies, label='Training Accuracy')\n","    plt.plot(validation_accuracies, label='Validation Accuracy')\n","    plt.title('Training & Validation Accuracy')\n","    plt.xlabel('Iterations')\n","    plt.ylabel('Accuracy')\n","    plt.legend()\n","\n","    plt.show()\n","\n","\n","\n","\n","def trainIters(encoder, decoder, num_epochs, train_pairs, val_pairs, learning_rate=0.01):\n","    start = time.time()\n","    training_losses = []\n","    validation_losses = []\n","    training_accuracies = []\n","    validation_accuracies = []\n","\n","    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n","    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n","    criterion = nn.CrossEntropyLoss()\n","\n","    for epoch in tqdm(range(1, num_epochs + 1), desc=\"Training Epochs\"):\n","        epoch_training_losses = []\n","        epoch_training_accuracies = []\n","\n","        for training_pair in train_pairs:\n","            input_tensor, target_tensor = tensorsFromPair(training_pair)\n","\n","            encoder_hidden = encoder.initHidden()\n","\n","            encoder_optimizer.zero_grad()\n","            decoder_optimizer.zero_grad()\n","\n","            input_length = input_tensor.size(0)\n","            target_length = target_tensor.size(0)\n","\n","            loss = 0\n","            correct = 0\n","\n","            for ei in range(input_length):\n","                encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n","\n","            # Adjust the hidden state for the decoder: concatenate the forward and backward states\n","            decoder_hidden = torch.cat((encoder_hidden[0:1], encoder_hidden[1:2]), 2)\n","\n","            decoder_input = torch.tensor([[SOS_token]], device=device)\n","\n","            use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n","\n","            for di in range(target_length):\n","                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n","                topv, topi = decoder_output.topk(1)\n","\n","                if topi.squeeze().item() == target_tensor[di].item():\n","                    correct += 1\n","\n","                loss += criterion(decoder_output, target_tensor[di])\n","                if use_teacher_forcing:\n","                    decoder_input = target_tensor[di]  # Teacher forcing\n","                else:\n","                    decoder_input = topi.squeeze().detach()\n","\n","                if decoder_input.item() == EOS_token:\n","                    break\n","\n","            loss.backward()\n","\n","            encoder_optimizer.step()\n","            decoder_optimizer.step()\n","\n","            accuracy = correct / float(target_length)\n","            epoch_training_losses.append(loss.item() / target_length)\n","            epoch_training_accuracies.append(accuracy)\n","\n","        avg_training_loss = sum(epoch_training_losses) / len(epoch_training_losses)\n","        avg_training_accuracy = sum(epoch_training_accuracies) / len(epoch_training_accuracies)\n","        training_losses.append(avg_training_loss)\n","        training_accuracies.append(avg_training_accuracy)\n","\n","        val_loss, val_accuracy = evaluate(encoder, decoder, val_pairs, criterion)\n","        validation_losses.append(val_loss)\n","        validation_accuracies.append(val_accuracy)\n","\n","        tqdm.write(f'End of Epoch {epoch} - Training Loss: {avg_training_loss:.4f}, Training Accuracy: {avg_training_accuracy * 100:.2f}%, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy * 100:.2f}%')\n","\n","    # Plotting training and validation metrics\n","    plot_metrics(training_losses, validation_losses, training_accuracies, validation_accuracies)\n","\n","    \n","\n","\n"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T06:58:36.289129Z","iopub.status.busy":"2023-12-04T06:58:36.288743Z","iopub.status.idle":"2023-12-04T06:58:38.956846Z","shell.execute_reply":"2023-12-04T06:58:38.955749Z","shell.execute_reply.started":"2023-12-04T06:58:36.289097Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['je suis d sol d avoir rat votre anniversaire .', 'i m sorry i missed your birthday .']\n","['quand on vit dans un pays il est evident quon doit egalement pouvoir comprendre la langue de ce pays', 'if you live in a country it is clear that you must also be able to understand the language of the country']\n","['nous avons raison devoquer dans notre resolution un soutien financier en faveur de la readaptation et de la reintegration des victimes', 'it is right and proper that in our resolution we also talk of money for the rehabilitation and reintegration of victims']\n","['je dois dire au commissairemichel que la facon normale de reagir a ce genre devenement serait de demissionner', 'i have to say to commissionermichel that the normal response to something like that would be to resign']\n","['j ai eu une enfance horrible .', 'i had a horrible childhood .']\n","Input tensor: tensor([[   54],\n","        [   55],\n","        [  136],\n","        [ 4379],\n","        [  136],\n","        [  331],\n","        [40140],\n","        [   78],\n","        [ 7170],\n","        [38154],\n","        [    1]], device='cuda:0')\n","Target tensor: tensor([[   46],\n","        [ 7991],\n","        [   49],\n","        [   46],\n","        [ 5581],\n","        [   69],\n","        [ 5921],\n","        [26734],\n","        [    1]], device='cuda:0')\n"]}],"source":["# Check the first few pairs\n","for i in range(5):\n","    print(train_pairs[i])\n","\n","# Test the tensor conversion on the first pair\n","input_tensor, target_tensor = tensorsFromPair(train_pairs[0])\n","print(\"Input tensor:\", input_tensor)\n","print(\"Target tensor:\", target_tensor)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T12:29:56.215444Z","iopub.status.busy":"2023-12-10T12:29:56.214791Z","iopub.status.idle":"2023-12-10T15:20:53.386122Z","shell.execute_reply":"2023-12-10T15:20:53.385096Z","shell.execute_reply.started":"2023-12-10T12:29:56.215412Z"},"trusted":true},"outputs":[],"source":["hidden_size = 256\n","\n","# Split your dataset into training, validation, and test sets\n","train_pairs, val_pairs, test_pairs = split_dataset(pairs, test_size=0.3, val_size=0.5)\n","\n","# Initialize your models\n","encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n","attn_decoder = DecoderRNN(hidden_size, output_lang.n_words).to(device)\n","\n","# Define the number of epochs\n","num_epochs = 20\n","\n","# Start the training process for the specified number of epochs\n","trainIters(encoder, attn_decoder, num_epochs, train_pairs, val_pairs)\n"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T15:29:54.359972Z","iopub.status.busy":"2023-12-10T15:29:54.359551Z","iopub.status.idle":"2023-12-10T15:30:37.227591Z","shell.execute_reply":"2023-12-10T15:30:37.226527Z","shell.execute_reply.started":"2023-12-10T15:29:54.359943Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Loss: 3.1276, Test Accuracy: 46.94%\n"]}],"source":["test_loss, test_accuracy = evaluate(encoder, attn_decoder, test_pairs, nn.CrossEntropyLoss(), MAX_LENGTH)\n","print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy * 100:.2f}%')"]},{"cell_type":"code","execution_count":53,"metadata":{"execution":{"iopub.execute_input":"2023-12-05T07:06:33.580904Z","iopub.status.busy":"2023-12-05T07:06:33.580607Z","iopub.status.idle":"2023-12-05T07:06:33.704424Z","shell.execute_reply":"2023-12-05T07:06:33.703468Z","shell.execute_reply.started":"2023-12-05T07:06:33.580878Z"},"trusted":true},"outputs":[],"source":["torch.save(encoder.state_dict(), 'BiD-GRUencoder_state.pth')\n","torch.save(attn_decoder.state_dict(), 'BiD-GRUdecoder_state.pth')"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T17:35:47.181796Z","iopub.status.busy":"2023-12-04T17:35:47.181434Z","iopub.status.idle":"2023-12-04T17:35:47.214234Z","shell.execute_reply":"2023-12-04T17:35:47.212854Z","shell.execute_reply.started":"2023-12-04T17:35:47.181766Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'training_losses' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plot_metrics(\u001b[43mtraining_losses\u001b[49m, validation_losses, training_accuracies, validation_accuracies)\n","\u001b[0;31mNameError\u001b[0m: name 'training_losses' is not defined"]}],"source":[" plot_metrics(training_losses, validation_losses, training_accuracies, validation_accuracies)"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T12:27:56.242108Z","iopub.status.busy":"2023-12-10T12:27:56.241317Z","iopub.status.idle":"2023-12-10T12:27:56.436373Z","shell.execute_reply":"2023-12-10T12:27:56.435049Z","shell.execute_reply.started":"2023-12-10T12:27:56.242066Z"},"trusted":true},"outputs":[{"ename":"RuntimeError","evalue":"Error(s) in loading state_dict for EncoderRNN:\n\tsize mismatch for embedding.weight: copying a param with shape torch.Size([34935, 256]) from checkpoint, the shape in current model is torch.Size([15457, 256]).","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[23], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Define the number of epochs\u001b[39;00m\n\u001b[1;32m     11\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15\u001b[39m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/kaggle/input/bid-gru/BiD-GRUencoder_state (2).pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m decoder\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/input/bid-gru/BiD-GRUdecoder_state (1).pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Start the training process for the specified number of epochs\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2036\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2037\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2038\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2042\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n","\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for EncoderRNN:\n\tsize mismatch for embedding.weight: copying a param with shape torch.Size([34935, 256]) from checkpoint, the shape in current model is torch.Size([15457, 256])."]}],"source":["# Load the model weights\n","hidden_size = 256\n","# Split your dataset into training, validation, and test sets\n","train_pairs, val_pairs, test_pairs = split_dataset(pairs, test_size=0.3, val_size=0.5)\n","\n","# Initialize your models\n","encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n","attn_decoder = DecoderRNN(hidden_size, output_lang.n_words).to(device)\n","\n","# Define the number of epochs\n","num_epochs = 15\n","\n","\n","encoder.load_state_dict(torch.load('/kaggle/input/bid-gru/BiD-GRUencoder_state (2).pth'))\n","decoder.load_state_dict(torch.load('/kaggle/input/bid-gru/BiD-GRUdecoder_state (1).pth'))\n","\n","# Start the training process for the specified number of epochs\n","trainIters(encoder, attn_decoder, num_epochs, train_pairs, val_pairs)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4141031,"sourceId":7167957,"sourceType":"datasetVersion"}],"dockerImageVersionId":30616,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
